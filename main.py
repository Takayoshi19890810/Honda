# -*- coding: utf-8 -*-
import os
import re
import io
import time
import argparse
from datetime import datetime, timezone, timedelta

import pandas as pd
import requests
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager


# ===== å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====
def jst_now():
    return datetime.now(timezone(timedelta(hours=9)))

def ym_tag():
    return jst_now().strftime("%Y-%m")

def monthly_excel_name():
    return f"yahoo_news_{jst_now().strftime('%Y-%m')}.xlsx"

DEFAULT_KEYWORD = "ãƒ›ãƒ³ãƒ€"   # NEWS_KEYWORD ç’°å¢ƒå¤‰æ•° or --keyword ã§ä¸Šæ›¸ã


# ===== Chromeï¼ˆheadlessï¼‰ =====
def make_driver() -> webdriver.Chrome:
    opts = Options()
    chrome_path = os.getenv("CHROME_PATH")  # Actionsã§æ³¨å…¥
    if chrome_path:
        opts.binary_location = chrome_path
    opts.add_argument("--headless=new")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--window-size=1280,2000")
    opts.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
    )
    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opts)


# ===== Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ =====
def format_dt(dt: datetime) -> str:
    return dt.strftime("%Y/%m/%d %H:%M")

def get_yahoo_news(keyword: str) -> pd.DataFrame:
    """
    Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆæ¤œç´¢ï¼‰ã‹ã‚‰ ã‚¿ã‚¤ãƒˆãƒ«/URL/æŠ•ç¨¿æ—¥/å¼•ç”¨å…ƒ ã‚’å–å¾—
    1ãƒšãƒ¼ã‚¸åˆ†ï¼ˆå¿…è¦ãªã‚‰ãƒšãƒ¼ã‚¸é€ã‚Šå‡¦ç†ã‚’è¿½åŠ å¯èƒ½ï¼‰
    """
    driver = make_driver()
    url = (
        f"https://news.yahoo.co.jp/search?p={keyword}"
        f"&ei=utf-8&categories=domestic,world,business,it,science,life,local"
    )
    driver.get(url)
    time.sleep(5)  # åˆæœŸæç”»å¾…ã¡

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    items = soup.find_all("li", class_=re.compile("sc-1u4589e-0"))
    rows = []
    for li in items:
        try:
            title_tag = li.find("div", class_=re.compile("sc-3ls169-0"))
            link_tag = li.find("a", href=True)
            time_tag = li.find("time")

            title = title_tag.get_text(strip=True) if title_tag else ""
            url = link_tag["href"] if link_tag else ""
            date_str = time_tag.get_text(strip=True) if time_tag else ""

            # æŠ•ç¨¿æ—¥æ­£è¦åŒ–
            pub_date = "å–å¾—ä¸å¯"
            if date_str:
                ds = re.sub(r'\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)', '', date_str).strip()
                try:
                    dt = datetime.strptime(ds, "%Y/%m/%d %H:%M")
                    pub_date = format_dt(dt)
                except Exception:
                    pub_date = ds

            # å¼•ç”¨å…ƒã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            source = ""
            for sel in [
                "div.sc-n3vj8g-0.yoLqH div.sc-110wjhy-8.bsEjY span",
                "div.sc-n3vj8g-0.yoLqH",
                "span",
                "div"
            ]:
                el = li.select_one(sel)
                if not el:
                    continue
                txt = el.get_text(" ", strip=True)

                # æ—¥ä»˜ï¼ˆYYYY/MM/DD HH:MM ã¾ãŸã¯ M/D H:MMï¼‰ã‚’å‰Šé™¤
                txt = re.sub(r"\d{1,4}/\d{1,2}/\d{1,2}\s+\d{1,2}:\d{2}", "", txt)
                txt = re.sub(r"\d{1,2}/\d{1,2}\s+\d{1,2}:\d{2}", "", txt)

                # ä¸¸æ‹¬å¼§å†…ã‚’å‰Šé™¤
                txt = re.sub(r"\([^)]+\)", "", txt)

                # å…ˆé ­ã®æ•°å­—ï¼‹ç©ºç™½ã‚’å‰Šé™¤ï¼ˆä¾‹: "2 Merkmal" â†’ "Merkmal"ï¼‰
                txt = re.sub(r"^\d+\s*", "", txt)

                txt = txt.strip()
                if txt and not txt.isdigit():
                    source = txt
                    break

            if title and url:
                rows.append({"ã‚¿ã‚¤ãƒˆãƒ«": title, "URL": url, "æŠ•ç¨¿æ—¥": pub_date, "å¼•ç”¨å…ƒ": source or "Yahoo"})
        except Exception:
            continue

    return pd.DataFrame(rows, columns=["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ"])


# ===== Releaseã‹ã‚‰æ—¢å­˜Excelã‚’å–å¾— =====
def download_existing_from_release(repo: str, tag: str, asset_name: str, token: str) -> pd.DataFrame:
    """Release(tag)ã«å­˜åœ¨ã™ã‚Œã°Excelã‚’DLã—ã¦DFã§è¿”ã™ã€‚ç„¡ã‘ã‚Œã°ç©ºDFã€‚"""
    if not (repo and tag and token):
        return pd.DataFrame(columns=["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ"])

    base = "https://api.github.com"
    headers = {"Authorization": f"Bearer {token}", "Accept": "application/vnd.github+json"}

    r = requests.get(f"{base}/repos/{repo}/releases/tags/{tag}", headers=headers)
    if r.status_code != 200:
        return pd.DataFrame(columns=["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ"])
    rel = r.json()

    asset = next((a for a in rel.get("assets", []) if a.get("name") == asset_name), None)
    if not asset:
        return pd.DataFrame(columns=["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ"])

    headers_dl = headers | {"Accept": "application/octet-stream"}
    dr = requests.get(asset["url"], headers=headers_dl)
    if dr.status_code != 200:
        return pd.DataFrame(columns=["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ"])

    with io.BytesIO(dr.content) as bio:
        try:
            df = pd.read_excel(bio, sheet_name="news")
            return df[["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ"]].copy()
        except Exception:
            return pd.DataFrame(columns=["ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ"])


# ===== ãƒ¡ã‚¤ãƒ³ =====
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--keyword", type=str, default=None, help="æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆæœªæŒ‡å®šãªã‚‰ç’°å¢ƒå¤‰æ•°NEWS_KEYWORDã€ãªã‘ã‚Œã°ãƒ›ãƒ³ãƒ€ï¼‰")
    args = ap.parse_args()

    keyword = args.keyword or os.getenv("NEWS_KEYWORD") or DEFAULT_KEYWORD
    print(f"ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keyword}")

    # 1) æœ€æ–°å–å¾—
    df_new = get_yahoo_news(keyword)

    # 2) æ—¢å­˜ï¼ˆåŒæœˆReleaseè³‡ç”£ï¼‰ã¨ãƒãƒ¼ã‚¸
    token = os.getenv("GITHUB_TOKEN", "")
    repo = os.getenv("GITHUB_REPOSITORY", "")
    tag = f"news-{ym_tag()}"
    asset = monthly_excel_name()

    df_old = download_existing_from_release(repo, tag, asset, token)
    df_all = pd.concat([df_old, df_new], ignore_index=True)

    if not df_all.empty:
        df_all = df_all.dropna(subset=["URL"]).drop_duplicates(subset=["URL"], keep="last")
        # æŠ•ç¨¿æ—¥ã§é™é †ï¼ˆå¤‰æ›ã§ããªã„å€¤ã¯æœ«å°¾ï¼‰
        try:
            dt = pd.to_datetime(df_all["æŠ•ç¨¿æ—¥"], errors="coerce", format="%Y/%m/%d %H:%M")
            df_all = df_all.assign(_dt=dt).sort_values("_dt", ascending=False).drop(columns=["_dt"])
        except Exception:
            pass

    # 3) ä¿å­˜ï¼ˆå˜ä¸€ã‚·ãƒ¼ãƒˆ newsï¼‰
    os.makedirs("output", exist_ok=True)
    out_path = os.path.join("output", monthly_excel_name())
    with pd.ExcelWriter(out_path, engine="openpyxl") as w:
        df_all.to_excel(w, index=False, sheet_name="news")

    print(f"âœ… Excelå‡ºåŠ›: {out_path}ï¼ˆåˆè¨ˆ {len(df_all)} ä»¶ã€ã†ã¡æ–°è¦ {len(df_new)} ä»¶ï¼‰")


if __name__ == "__main__":
    main()
