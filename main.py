import pandas as pd
import time
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from datetime import datetime, timedelta
import re
from openpyxl import load_workbook
from openpyxl.worksheet.table import Table, TableStyleInfo
import os
import json # gspreadã®èªè¨¼æƒ…å ±ã®ãŸã‚

# âœ… ç¾åœ¨æ™‚åˆ»ï¼ˆJSTï¼‰ - å…¨ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ã§ä½¿ç”¨
now = datetime.utcnow() + timedelta(hours=9)

# âœ… æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°ï¼‰ - MSNãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ä½¿ç”¨
KEYWORDS_MSN = ["Honda", "HONDA", "ãƒ›ãƒ³ãƒ€"]

# âœ… Google/Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ç”¨ã®å˜ä¸€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
KEYWORD_SINGLE = "ãƒ›ãƒ³ãƒ€"

# âœ… ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆè¨­å®š
SPREADSHEET_ID = "1AwwMGKMHfduwPkrtsik40lkO1z1T8IU_yd41ku-yPi8"

def format_datetime(dt_obj):
    """datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®æ–‡å­—åˆ—ã«å¤‰æ›ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"""
    return dt_obj.strftime("%Y/%m/%d %H:%M")

def get_google_news_with_selenium(keyword: str) -> list[dict]:
    """Googleãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ã™ã‚‹é–¢æ•°"""
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    url = f"https://news.google.com/search?q={keyword}&hl=ja&gl=JP&ceid=JP:ja"
    driver.get(url)
    time.sleep(5)
    # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦ã‚ˆã‚Šå¤šãã®è¨˜äº‹ã‚’ãƒ­ãƒ¼ãƒ‰
    for _ in range(3):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()

    articles = soup.find_all("article")
    data = []
    for article in articles:
        try:
            a_tag = article.select_one("a.JtKRv")
            time_tag = article.select_one("time.hvbAAd")
            source_tag = article.select_one("div.vr1PYe")
            title = a_tag.text.strip()
            href = a_tag.get("href")
            # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
            url = "https://news.google.com" + href[1:] if href.startswith("./") else href
            # æ—¥æ™‚ã‚’JSTã«å¤‰æ›
            dt = datetime.strptime(time_tag.get("datetime"), "%Y-%m-%dT%H:%M:%SZ") + timedelta(hours=9)
            pub_date = format_datetime(dt)
            source = source_tag.text.strip() if source_tag else "N/A"
            data.append({"ã‚¿ã‚¤ãƒˆãƒ«": title, "URL": url, "æŠ•ç¨¿æ—¥": pub_date, "å¼•ç”¨å…ƒ": source})
        except Exception as e:
            # print(f"âš ï¸ Googleè¨˜äº‹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}") # ãƒ‡ãƒãƒƒã‚°ç”¨
            continue
    print(f"âœ… Googleãƒ‹ãƒ¥ãƒ¼ã‚¹ä»¶æ•°: {len(data)} ä»¶")
    return data

def get_yahoo_news_with_selenium(keyword: str) -> list[dict]:
    """Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ã™ã‚‹é–¢æ•°"""
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    search_url = f"https://news.yahoo.co.jp/search?p={keyword}&ei=utf-8"
    driver.get(search_url)
    time.sleep(5)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    driver.quit()
    # è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒŠã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’èª¿æ•´
    articles = soup.find_all("li", class_=re.compile("sc-1u4589e-0"))
    data = []

    for article in articles:
        try:
            title_tag = article.find("div", class_=re.compile("sc-3ls169-0"))
            title = title_tag.text.strip() if title_tag else ""
            link_tag = article.find("a", href=True)
            url = link_tag["href"] if link_tag else ""
            time_tag = article.find("time")
            date_str = time_tag.text.strip() if time_tag else ""
            formatted_date = ""
            if date_str:
                date_str = re.sub(r'\([æœˆç«æ°´æœ¨é‡‘åœŸæ—¥]\)', '', date_str).strip() # æ›œæ—¥ã‚’å‰Šé™¤
                try:
                    dt_obj = datetime.strptime(date_str, "%Y/%m/%d %H:%M")
                    formatted_date = format_datetime(dt_obj)
                except ValueError:
                    formatted_date = date_str # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ããªã„å ´åˆã¯ãã®ã¾ã¾
            source_text = "Yahoo!"
            if title and url:
                data.append({
                    "ã‚¿ã‚¤ãƒˆãƒ«": title,
                    "URL": url,
                    "æŠ•ç¨¿æ—¥": formatted_date if formatted_date else "å–å¾—ä¸å¯",
                    "å¼•ç”¨å…ƒ": source_text
                })
        except Exception as e:
            # print(f"âš ï¸ Yahoo!è¨˜äº‹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}") # ãƒ‡ãƒãƒƒã‚°ç”¨
            continue

    print(f"âœ… Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ä»¶æ•°: {len(data)} ä»¶")
    return data

def get_msn_news_with_selenium(keywords: list[str]) -> list[dict]:
    """MSNãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ã™ã‚‹é–¢æ•°ï¼ˆè¤‡æ•°ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å¯¾å¿œï¼‰"""
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")

    # driverã®åˆæœŸåŒ–ã¯ãƒ«ãƒ¼ãƒ—ã®å¤–ã§è¡Œã„ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã”ã¨ã«ãƒšãƒ¼ã‚¸ã‚’ãƒ­ãƒ¼ãƒ‰
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    all_msn_data = []

    for keyword in keywords:
        print(f"ğŸ” MSNãƒ‹ãƒ¥ãƒ¼ã‚¹ - å‡¦ç†ä¸­: {keyword}")
        search_url = f'https://www.bing.com/news/search?q={keyword}&qft=sortbydate%3d"1"&form=YFNR'
        driver.get(search_url)
        time.sleep(5) # ãƒšãƒ¼ã‚¸ãŒå®Œå…¨ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã‚ˆã†ã«å°‘ã—é•·ã‚ã«å¾…ã¤

        soup = BeautifulSoup(driver.page_source, "html.parser")

        # div.news-card ã‚’æ¢ã—ã€ãƒ‡ãƒ¼ã‚¿å±æ€§ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
        for card in soup.select('div.news-card'):
            title = card.get("data-title", "").strip()
            url = card.get("data-url", "").strip()
            source = card.get("data-author", "").strip()

            pub_time_obj = None
            pub_label = ""

            # æŠ•ç¨¿æ™‚é–“ã‚’aria-labelå±æ€§ã‹ã‚‰å–å¾—
            pub_tag = card.find("span", attrs={"aria-label": True})
            if pub_tag and pub_tag.has_attr("aria-label"):
                pub_label = pub_tag["aria-label"].strip()

            # ç›¸å¯¾æ™‚é–“è¡¨è¨˜ã‚’datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯
            if "åˆ†å‰" in pub_label:
                m = re.search(r"(\d+)", pub_label)
                if m:
                    pub_time_obj = now - timedelta(minutes=int(m.group(1)))
            elif "æ™‚é–“å‰" in pub_label:
                h = re.search(r"(\d+)", pub_label)
                if h:
                    pub_time_obj = now - timedelta(hours=int(h.group(1)))
            elif "æ—¥å‰" in pub_label:
                d = re.search(r"(\d+)", pub_label)
                if d:
                    pub_time_obj = now - timedelta(days=int(d.group(1)))
            elif re.match(r'\d+æœˆ\d+æ—¥', pub_label): # ä¾‹: 1æœˆ1æ—¥
                try:
                    # å¹´ã¯ç¾åœ¨ã®å¹´ã‚’ä½¿ç”¨ã™ã‚‹ã¨ä»®å®š
                    pub_time_obj = datetime.strptime(f"{now.year}å¹´{pub_label}", "%Yå¹´%mæœˆ%dæ—¥")
                except:
                    pub_time_obj = None
            elif re.match(r'\d{4}/\d{1,2}/\d{1,2}', pub_label): # ä¾‹: 2024/01/01
                try:
                    pub_time_obj = datetime.strptime(pub_label, "%Y/%m/%d")
                except:
                    pub_time_obj = None
            elif re.match(r'\d{1,2}:\d{2}', pub_label): # ä¾‹: 15:30 (ä»Šæ—¥ã®æ—¥ä»˜ã¨çµåˆ)
                try:
                    t = datetime.strptime(pub_label, "%H:%M").time()
                    pub_time_obj = datetime.combine(now.date(), t)
                except:
                    pub_time_obj = None

            pub_date = pub_time_obj.strftime("%Y/%m/%d %H:%M") if pub_time_obj else pub_label

            if title and url:
                all_msn_data.append({
                    "ã‚¿ã‚¤ãƒˆãƒ«": title,
                    "URL": url,
                    "æŠ•ç¨¿æ—¥": pub_date,
                    "å¼•ç”¨å…ƒ": source if source else "MSN"
                })
        # print(f"âœ… MSNãƒ‹ãƒ¥ãƒ¼ã‚¹ - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ '{keyword}' ã§ {len(all_msn_data)} ä»¶ã®è¨˜äº‹ã‚’ç™ºè¦‹ã€‚") # ãƒ‡ãƒãƒƒã‚°ç”¨
    
    driver.quit() # å…¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡¦ç†å¾Œã«ä¸€åº¦ã ã‘ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’çµ‚äº†
    print(f"âœ… MSNãƒ‹ãƒ¥ãƒ¼ã‚¹ç·ä»¶æ•°: {len(all_msn_data)} ä»¶")
    return all_msn_data

def write_to_spreadsheet(articles: list[dict], spreadsheet_id: str, worksheet_name: str):
    """è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã‚€é–¢æ•°"""
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èªè¨¼æƒ…å ±ã‚’å–å¾—
    credentials_json_str = os.environ.get('GCP_SERVICE_ACCOUNT_KEY')
    # ç’°å¢ƒå¤‰æ•°ãŒãªã„å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç”¨ï¼‰
    if credentials_json_str:
        credentials = json.loads(credentials_json_str)
    else:
        # ãƒ­ãƒ¼ã‚«ãƒ«ã§ã®ãƒ†ã‚¹ãƒˆæ™‚ã« credentials.json ãŒå¿…è¦
        # æœ¬ç•ªç’°å¢ƒã§ã¯ç’°å¢ƒå¤‰æ•° GCP_SERVICE_ACCOUNT_KEY ã«JSONæ–‡å­—åˆ—ã‚’è¨­å®šã—ã¦ãã ã•ã„
        try:
            with open('credentials.json', 'r') as f:
                credentials = json.load(f)
        except FileNotFoundError:
            raise RuntimeError("èªè¨¼ãƒ•ã‚¡ã‚¤ãƒ« 'credentials.json' ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€GCP_SERVICE_ACCOUNT_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    gc = gspread.service_account_from_dict(credentials)

    for attempt in range(5): # è¤‡æ•°å›ãƒªãƒˆãƒ©ã‚¤
        try:
            sh = gc.open_by_key(spreadsheet_id)
            try:
                worksheet = sh.worksheet(worksheet_name)
            except gspread.exceptions.WorksheetNotFound:
                # ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆãŒå­˜åœ¨ã—ãªã„å ´åˆã¯æ–°è¦ä½œæˆã—ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¿½åŠ 
                worksheet = sh.add_worksheet(title=worksheet_name, rows="1", cols="4")
                worksheet.append_row(['ã‚¿ã‚¤ãƒˆãƒ«', 'URL', 'æŠ•ç¨¿æ—¥', 'å¼•ç”¨å…ƒ'])

            existing_data = worksheet.get_all_values()
            # æ—¢å­˜ã®URLã‚’ã‚»ãƒƒãƒˆã«æ ¼ç´ã—ã¦é‡è¤‡ãƒã‚§ãƒƒã‚¯ã‚’é«˜é€ŸåŒ–
            existing_urls = set(row[1] for row in existing_data[1:] if len(row) > 1)

            # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            new_data = [[a['ã‚¿ã‚¤ãƒˆãƒ«'], a['URL'], a['æŠ•ç¨¿æ—¥'], a['å¼•ç”¨å…ƒ']] for a in articles if a['URL'] not in existing_urls]
            
            if new_data:
                # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«è¿½è¨˜
                worksheet.append_rows(new_data, value_input_option='USER_ENTERED')
                print(f"âœ… {len(new_data)}ä»¶ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«è¿½è¨˜ã—ã¾ã—ãŸã€‚")
            else:
                print("âš ï¸ è¿½è¨˜ã™ã¹ãæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
            return # æˆåŠŸã—ãŸã‚‰é–¢æ•°ã‚’çµ‚äº†
        except gspread.exceptions.APIError as e:
            print(f"âš ï¸ Google API Error (attempt {attempt + 1}/5): {e}")
            time.sleep(5 + random.random() * 5) # ãƒªãƒˆãƒ©ã‚¤å‰ã«å¾…æ©Ÿ
    
    # 5å›è©¦è¡Œã—ã¦ã‚‚æˆåŠŸã—ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
    raise RuntimeError("âŒ Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®æ›¸ãè¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆ5å›è©¦è¡Œã—ã¦ã‚‚æˆåŠŸã›ãšï¼‰")

if __name__ == "__main__":
    print("\n--- Google News ---")
    google_news_articles = get_google_news_with_selenium(KEYWORD_SINGLE)
    if google_news_articles:
        write_to_spreadsheet(google_news_articles, SPREADSHEET_ID, "Google")
    
    print("\n--- Yahoo! News ---")
    yahoo_news_articles = get_yahoo_news_with_selenium(KEYWORD_SINGLE)
    if yahoo_news_articles:
        write_to_spreadsheet(yahoo_news_articles, SPREADSHEET_ID, "Yahoo")

    # MSNãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å‡¦ç†ã¯ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã§æ¸¡ã™
    print("\n--- MSN News ---")
    msn_news_articles = get_msn_news_with_selenium(KEYWORDS_MSN)
    # ã“ã“ã§é‡è¤‡æ’é™¤ (get_msn_news_with_selenium å†…ã§ã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã”ã¨ã®é‡è¤‡æ’é™¤ã¯ã—ã¦ã„ãªã„ãŸã‚)
    df_msn = pd.DataFrame(msn_news_articles)
    if not df_msn.empty:
        df_msn.drop_duplicates(subset=["URL"], inplace=True)
        write_to_spreadsheet(df_msn.to_dict('records'), SPREADSHEET_ID, "MSN")
    else:
        print("âš ï¸ MSNãƒ‹ãƒ¥ãƒ¼ã‚¹ã®è¿½è¨˜ã™ã¹ãæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    
    print("\n--- å…¨ã¦ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ã®æŠ½å‡ºã¨ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®æ›¸ãè¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸ ---")

    # ãƒ­ãƒ¼ã‚«ãƒ«Excelãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®æ›¸ãå‡ºã—å‡¦ç†ï¼ˆå…ƒã®MSNå°‚ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‹ã‚‰çµ±åˆï¼‰
    # å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã¦é‡è¤‡æ’é™¤
    combined_all_data = google_news_articles + yahoo_news_articles + msn_news_articles # å‹ãŒdictã®ãƒªã‚¹ãƒˆã§ã‚ã‚‹ã“ã¨ã‚’å‰æ

    # combined_all_data ã‚’ DataFrame ã«å¤‰æ›ã—ã¦é‡è¤‡æ’é™¤
    df_combined = pd.DataFrame(combined_all_data)
    if not df_combined.empty:
        df_combined.drop_duplicates(subset=["URL"], inplace=True)
    
        output_file = "all_news_summary.xlsx" # çµ±åˆã•ã‚ŒãŸå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
        df_combined.to_excel(output_file, index=False)

        # ãƒ•ã‚£ãƒ«ã‚¿ä»˜ããƒ†ãƒ¼ãƒ–ãƒ«è¿½åŠ 
        try:
            wb = load_workbook(output_file)
            ws = wb.active

            end_row = ws.max_row
            end_col = ws.max_column
            end_col_letter = chr(ord('A') + end_col - 1)
            table_range = f"A1:{end_col_letter}{end_row}"

            table = Table(displayName="CombinedNewsTable", ref=table_range)
            style = TableStyleInfo(name="TableStyleLight1", showRowStripes=False)
            table.tableStyleInfo = style
            ws.add_table(table)
            wb.save(output_file)
            print("âœ… çµ±åˆExcelãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ•ã‚£ãƒ«ã‚¿ä»˜ããƒ†ãƒ¼ãƒ–ãƒ«ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"âš ï¸ çµ±åˆExcelãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ã‚£ãƒ«ã‚¿è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")

        print(f"âœ… å…¨ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ã®æŠ½å‡ºå®Œäº†: {output_file}ï¼ˆä»¶æ•°: {len(df_combined)}ï¼‰")
    else:
        print("âš ï¸ å…¨ã¦ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚½ãƒ¼ã‚¹ã§æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
