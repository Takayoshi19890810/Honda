import pandas as pd
import time
import os
import re
import json
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
import gspread

# âœ… å®šæ•°
KEYWORDS = ["Honda", "HONDA", "ãƒ›ãƒ³ãƒ€"]
SPREADSHEET_ID = "1AwwMGKMHfduwPkrtsik40lkO1z1T8IU_yd41ku-yPi8"
SHEET_NAME = "MSN"
JST = datetime.utcnow() + timedelta(hours=9)

# âœ… Seleniumãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•
def start_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

# âœ… ç›¸å¯¾æ™‚é–“ â†’ çµ¶å¯¾æ—¥æ™‚ã¸å¤‰æ›
def parse_pub_date(label: str) -> str:
    try:
        label = label.strip()
        if "åˆ†å‰" in label:
            return (JST - timedelta(minutes=int(re.search(r"\d+", label)[0]))).strftime("%Y/%m/%d %H:%M")
        elif "æ™‚é–“å‰" in label:
            return (JST - timedelta(hours=int(re.search(r"\d+", label)[0]))).strftime("%Y/%m/%d %H:%M")
        elif "æ—¥å‰" in label:
            return (JST - timedelta(days=int(re.search(r"\d+", label)[0]))).strftime("%Y/%m/%d %H:%M")
        elif re.match(r'\d+æœˆ\d+æ—¥', label):
            dt = datetime.strptime(f"{JST.year}å¹´{label}", "%Yå¹´%mæœˆ%dæ—¥")
            return dt.strftime("%Y/%m/%d %H:%M")
        elif re.match(r'\d{4}/\d{1,2}/\d{1,2}', label):
            dt = datetime.strptime(label, "%Y/%m/%d")
            return dt.strftime("%Y/%m/%d %H:%M")
        elif re.match(r'\d{1,2}:\d{2}', label):
            t = datetime.strptime(label, "%H:%M").time()
            dt = datetime.combine(JST.date(), t)
            return dt.strftime("%Y/%m/%d %H:%M")
    except:
        pass
    return label

# âœ… MSNãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—
def get_msn_news(keyword: str, driver) -> list:
    print(f"ğŸ” æ¤œç´¢ä¸­: {keyword}")
    url = f'https://www.bing.com/news/search?q={keyword}&qft=sortbydate%3d"1"&form=YFNR'
    driver.get(url)
    time.sleep(5)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    data = []
    for card in soup.select("div.news-card"):
        title = card.get("data-title", "").strip()
        url = card.get("data-url", "").strip()
        source = card.get("data-author", "").strip()

        pub_label = ""
        tag = card.find("span", attrs={"aria-label": True})
        if tag and tag.has_attr("aria-label"):
            pub_label = tag["aria-label"].strip()

        pub_date = parse_pub_date(pub_label)

        if title and url:
            data.append({
                "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": keyword,
                "ã‚¿ã‚¤ãƒˆãƒ«": title,
                "URL": url,
                "æŠ•ç¨¿æ—¥": pub_date,
                "å¼•ç”¨å…ƒ": source if source else "MSN"
            })
    return data

# âœ… ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ›¸ãè¾¼ã¿
def write_to_spreadsheet(articles: list):
    print("ğŸ“¥ ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«æ›¸ãè¾¼ã¿ä¸­...")
    
    if not articles:
        print("âš ï¸ æ›¸ãè¾¼ã‚€è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
    
    # èªè¨¼ï¼ˆcredentials.json or ç’°å¢ƒå¤‰æ•°ï¼‰
    if os.path.exists("credentials.json"):
        gc = gspread.service_account(filename="credentials.json")
    else:
        credentials = json.loads(os.environ["GCP_SERVICE_ACCOUNT_KEY"])
        gc = gspread.service_account_from_dict(credentials)

    sh = gc.open_by_key(SPREADSHEET_ID)

    try:
        ws = sh.worksheet(SHEET_NAME)
    except gspread.exceptions.WorksheetNotFound:
        ws = sh.add_worksheet(title=SHEET_NAME, rows="1", cols="10")
        ws.append_row(["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "ã‚¿ã‚¤ãƒˆãƒ«", "URL", "æŠ•ç¨¿æ—¥", "å¼•ç”¨å…ƒ"])

    existing = ws.get_all_values()
    existing_urls = set(row[2] for row in existing[1:] if len(row) > 2)

    new_rows = [
        [a["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"], a["ã‚¿ã‚¤ãƒˆãƒ«"], a["URL"], a["æŠ•ç¨¿æ—¥"], a["å¼•ç”¨å…ƒ"]]
        for a in articles if a["URL"] not in existing_urls
    ]

    if new_rows:
        ws.append_rows(new_rows, value_input_option="USER_ENTERED")
        print(f"âœ… {len(new_rows)}ä»¶ã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«è¿½è¨˜ã—ã¾ã—ãŸã€‚")
    else:
        print("âš ï¸ ã™ã¹ã¦æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ãŸã‚ã€è¿½è¨˜ãªã—ã€‚")

# âœ… ãƒ¡ã‚¤ãƒ³å‡¦ç†
def main():
    driver = start_driver()
    all_articles = []
    for kw in KEYWORDS:
        all_articles.extend(get_msn_news(kw, driver))
    driver.quit()

    df = pd.DataFrame(all_articles)
    df.drop_duplicates(subset="URL", inplace=True)
    write_to_spreadsheet(df.to_dict("records"))
    print(f"âœ… å…¨å‡¦ç†å®Œäº†ï¼ˆåˆè¨ˆ: {len(df)} ä»¶ï¼‰")

if __name__ == "__main__":
    main()
